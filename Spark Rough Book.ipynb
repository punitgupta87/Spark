{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"UIDAI\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WINPG186028-3FP.TD.TERADATA.COM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d8cfbeb710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o261.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.nio.channels.ClosedChannelException\r\n\tat org.apache.spark.network.client.StreamInterceptor.channelInactive(StreamInterceptor.java:60)\r\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:179)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1354)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:917)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:822)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\r\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:148)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:201)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:594)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.nio.channels.ClosedChannelException\r\n\tat org.apache.spark.network.client.StreamInterceptor.channelInactive(StreamInterceptor.java:60)\r\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:179)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1354)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:917)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:822)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\r\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ef84543cf671>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                        \u001b[0mnullvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"N/A\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                        mode = \"failfast\").\\\n\u001b[1;32m----> 5\u001b[1;33m                        \u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/pg186028/Documents/GitHub/aadhaar-dataset-analysis/data/UIDAI-ENR-DETAIL-20170308.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o261.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.nio.channels.ClosedChannelException\r\n\tat org.apache.spark.network.client.StreamInterceptor.channelInactive(StreamInterceptor.java:60)\r\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:179)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1354)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:917)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:822)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\r\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:148)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:201)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:594)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.nio.channels.ClosedChannelException\r\n\tat org.apache.spark.network.client.StreamInterceptor.channelInactive(StreamInterceptor.java:60)\r\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:179)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1354)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:917)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:822)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\r\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "uid=spark.read.options(header=\"true\",\\\n",
    "                       inferschema=\"true\",\\\n",
    "                       nullvalue=\"N/A\",\\\n",
    "                       mode = \"failfast\").\\\n",
    "                       csv(\"C:/Users/pg186028/Documents/GitHub/aadhaar-dataset-analysis/data/UIDAI-ENR-DETAIL-20170308.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440818"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Registrar', 'string'),\n",
       " ('Enrolment Agency', 'string'),\n",
       " ('State', 'string'),\n",
       " ('District', 'string'),\n",
       " ('Sub District', 'string'),\n",
       " ('Pin Code', 'string'),\n",
       " ('Gender', 'string'),\n",
       " ('Age', 'int'),\n",
       " ('Aadhaar generated', 'int'),\n",
       " ('Enrolment Rejected', 'int'),\n",
       " ('Residents providing email', 'int'),\n",
       " ('Residents providing mobile number', 'int')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid.createOrReplaceTempView(\"uid_t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Registrar',\n",
       " 'Enrolment Agency',\n",
       " 'State',\n",
       " 'District',\n",
       " 'Sub District',\n",
       " 'Pin Code',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Aadhaar generated',\n",
       " 'Enrolment Rejected',\n",
       " 'Residents providing email',\n",
       " 'Residents providing mobile number']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "|               State|sum(Aadhaar generated)|\n",
      "+--------------------+----------------------+\n",
      "|           Karnataka|                 19764|\n",
      "|              Odisha|                 18182|\n",
      "|      Madhya Pradesh|                 53276|\n",
      "|         Maharashtra|                 26085|\n",
      "|           Telangana|                  5018|\n",
      "|         West Bengal|                119901|\n",
      "|        Chhattisgarh|                  6604|\n",
      "|             Haryana|                  6804|\n",
      "|   Jammu and Kashmir|                  1234|\n",
      "|   Arunachal Pradesh|                   913|\n",
      "|             Manipur|                  1323|\n",
      "|              Others|                    12|\n",
      "|               Bihar|                162607|\n",
      "|              Kerala|                 15143|\n",
      "|             Mizoram|                  6279|\n",
      "|              Sikkim|                    50|\n",
      "|      Andhra Pradesh|                  5798|\n",
      "|           Rajasthan|                 39570|\n",
      "|Andaman and Nicob...|                     5|\n",
      "|           Jharkhand|                  9868|\n",
      "+--------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select State\\\n",
    "            ,sum(`Aadhaar generated`) from uid_t group by State\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nagaland</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karnataka</td>\n",
       "      <td>19764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Odisha</td>\n",
       "      <td>18182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kerala</td>\n",
       "      <td>15143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>32485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>6604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>5798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lakshadweep</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>53276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Punjab</td>\n",
       "      <td>6506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Manipur</td>\n",
       "      <td>1323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Goa</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mizoram</td>\n",
       "      <td>6279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dadra and Nagar Haveli</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>1547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Puducherry</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>6804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Jammu and Kashmir</td>\n",
       "      <td>1234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>9868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>34844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sikkim</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>8426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Others</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>39570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Andaman and Nicobar Islands</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Assam</td>\n",
       "      <td>3213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Daman and Diu</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>26085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Telangana</td>\n",
       "      <td>5018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>West Bengal</td>\n",
       "      <td>119901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Bihar</td>\n",
       "      <td>162607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Tripura</td>\n",
       "      <td>908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>103767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>13227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          State   total\n",
       "0                      Nagaland     545\n",
       "1                     Karnataka   19764\n",
       "2                        Odisha   18182\n",
       "3                        Kerala   15143\n",
       "4                    Tamil Nadu   32485\n",
       "5                  Chhattisgarh    6604\n",
       "6                Andhra Pradesh    5798\n",
       "7                   Lakshadweep       4\n",
       "8                Madhya Pradesh   53276\n",
       "9                        Punjab    6506\n",
       "10                      Manipur    1323\n",
       "11                          Goa    1167\n",
       "12                      Mizoram    6279\n",
       "13       Dadra and Nagar Haveli     140\n",
       "14             Himachal Pradesh    1547\n",
       "15                   Puducherry      83\n",
       "16                      Haryana    6804\n",
       "17            Jammu and Kashmir    1234\n",
       "18                    Jharkhand    9868\n",
       "19            Arunachal Pradesh     913\n",
       "20                      Gujarat   34844\n",
       "21                       Sikkim      50\n",
       "22                        Delhi    8426\n",
       "23                       Others      12\n",
       "24                   Chandigarh     259\n",
       "25                    Rajasthan   39570\n",
       "26  Andaman and Nicobar Islands       5\n",
       "27                        Assam    3213\n",
       "28                    Meghalaya     277\n",
       "29                Daman and Diu     105\n",
       "30                  Maharashtra   26085\n",
       "31                    Telangana    5018\n",
       "32                  West Bengal  119901\n",
       "33                        Bihar  162607\n",
       "34                      Tripura     908\n",
       "35                Uttar Pradesh  103767\n",
       "36                  Uttarakhand   13227"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid2=uid.groupBy(\"State\").sum(\"Aadhaar generated\")\n",
    "uid3=uid2.withColumnRenamed(\"sum(Aadhaar generated)\",\"total\").toPandas()\n",
    "uid3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"To read the first item from the file, you can use the following command\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of words in text :  14\n",
      "Total Number of distinct words :  12\n"
     ]
    }
   ],
   "source": [
    "test_rdd=spark.sparkContext.parallelize(text,5)   #parallelize is one of the method to create RDD \n",
    "test_rdd_count=test_rdd.count()  # count is an action on RDD that we have created\n",
    "test_rdd_distinct=test_rdd.groupBy(lambda x:x).count()\n",
    "test_rdd.glom().collect()\n",
    "print(\"Total Number of words in text : \", test_rdd_count)\n",
    "print(\"Total Number of distinct words : \", test_rdd_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=test_rdd.filter(\"read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file,', 1),\n",
       " ('following', 1),\n",
       " ('read', 1),\n",
       " ('you', 1),\n",
       " ('the', 3),\n",
       " ('use', 1),\n",
       " ('command', 1),\n",
       " ('To', 1),\n",
       " ('first', 1),\n",
       " ('item', 1),\n",
       " ('from', 1),\n",
       " ('can', 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WINPG186028-3FP.TD.TERADATA.COM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x182659727b8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.read.load(\"C:/Users/pg186028/Documents/sample_data.csv\",format=\"csv\",\\\n",
    "                     header=False,\n",
    "                     inferschema=True,\\\n",
    "                     nullvalue=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|   _c0| _c1|\n",
      "+------+----+\n",
      "| Punit| 100|\n",
      "| Punit| 200|\n",
      "|Vishal| 300|\n",
      "|Vishal| 500|\n",
      "| Nitin|null|\n",
      "| Nitin| 700|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data.groupBy(\"_c0\").sum(\"_c1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Punit</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vishal</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nitin</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  total\n",
       "0   Punit    300\n",
       "1  Vishal    800\n",
       "2   Nitin    700"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.withColumnRenamed('_c0','name').withColumnRenamed('sum(_c1)','total').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_p=data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_p2=d_p.repartition(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(_c0='Punit', _c1=100),\n",
       "  Row(_c0='Punit', _c1=200),\n",
       "  Row(_c0='Vishal', _c1=300),\n",
       "  Row(_c0='Vishal', _c1=500),\n",
       "  Row(_c0='Nitin', _c1=1000),\n",
       "  Row(_c0='Nitin', _c1=700)]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_p.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Punit', 300), ('Vishal', 800), ('Nitin', 1700)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_p.map(lambda x: (x[0],x[1])).reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_p2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_p2=d_p.flatMap(lambda x : (x[0],x[1])).partitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Punit',\n",
       " 100,\n",
       " 'Punit',\n",
       " 200,\n",
       " 'Vishal',\n",
       " 300,\n",
       " 'Vishal',\n",
       " 500,\n",
       " 'Nitin',\n",
       " 1000,\n",
       " 'Nitin',\n",
       " 700]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WINPG186028-3FP.TD.TERADATA.COM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WINPG186028-3FP.TD.TERADATA.COM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x208774b27b8>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|   _c0| _c1|\n",
      "+------+----+\n",
      "| Punit| 100|\n",
      "| Punit| 200|\n",
      "|Vishal| 300|\n",
      "|Vishal| 500|\n",
      "| Nitin|null|\n",
      "| Nitin| 700|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|   _c0|_c1|\n",
      "+------+---+\n",
      "| Punit|100|\n",
      "| Punit|200|\n",
      "|Vishal|300|\n",
      "|Vishal|500|\n",
      "| Nitin|  0|\n",
      "| Nitin|700|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WINPG186028-3FP.TD.TERADATA.COM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2=spark.newSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.app.name', 'test'),\n",
       " ('spark.app.id', 'local-1561392756284'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '56758'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', 'WINPG186028-3FP.TD.TERADATA.COM'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySpark =SparkSession.builder.appName(\"Spark SQL basic example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1=SparkSession.builder.appName(\"UIDAI\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1=SparkSession.builder.appName(\"Test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=sc1.sparkContext.textFile(\"C:/Users/pg186028/Documents/sample_data.csv\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Punit,100', 'Punit,200', 'Vishal,300', 'Vishal,500', 'Nitin,', 'Nitin,700']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2=dt.map(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Punit',\n",
       " '100',\n",
       " 'Punit',\n",
       " '200',\n",
       " 'Vishal',\n",
       " '300',\n",
       " 'Vishal',\n",
       " '500',\n",
       " 'Nitin',\n",
       " '',\n",
       " 'Nitin',\n",
       " '700']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.flatMap(lambda x:x.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt3=dt2.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtx=spark.sparkContext.parallelize(dt2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Punit', '100'],\n",
       " ['Punit', '200'],\n",
       " ['Vishal', '300'],\n",
       " ['Vishal', '500'],\n",
       " ['Nitin', ''],\n",
       " ['Nitin', '700']]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtx.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1=SparkSession.builder.appName(\"Test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sc1.sparkContext.textFile(\"C:/Users/pg186028/Documents/sample_data.csv\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=a.map(lambda x : x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a11=a1.map(lambda x : (x[0],0) if x[1]=='' else (x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a12=a11.map(lambda  x : (x[0],int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Punit', 100),\n",
       " ('Punit', 200),\n",
       " ('Vishal', 300),\n",
       " ('Vishal', 500),\n",
       " ('Nitin', 0),\n",
       " ('Nitin', 700)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a12.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a13=spark.sparkContext.parallelize(a12.collect(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Punit', 100),\n",
       " ('Punit', 200),\n",
       " ('Vishal', 300),\n",
       " ('Vishal', 500),\n",
       " ('Nitin', 0),\n",
       " ('Nitin', 700)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a13.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Punit', 100),\n",
       " ('Punit', 200),\n",
       " ('Vishal', 300),\n",
       " ('Vishal', 500),\n",
       " ('Nitin', 0),\n",
       " ('Nitin', 700)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a13.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Punit', 100),\n",
       " ('Punit', 200),\n",
       " ('Vishal', 300),\n",
       " ('Vishal', 500),\n",
       " ('Nitin', 0),\n",
       " ('Nitin', 700)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a13.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Punit', 100), ('Punit', 200)],\n",
       " [('Vishal', 300), ('Vishal', 500)],\n",
       " [('Nitin', 0), ('Nitin', 700)]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a13.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a14=a13.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a14.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream.csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"Split the lines into words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f4d3ba16eb84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'explode' is not defined"
     ]
    }
   ],
   "source": [
    "words = x.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'the', 'lines', 'into', 'words']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=spark.sparkContext.parallelize(txt.split(\" \"),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'the', 'lines', 'into', 'words']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x.map(lambda x : (x,)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   _1|\n",
      "+-----+\n",
      "|Split|\n",
      "|  the|\n",
      "|lines|\n",
      "| into|\n",
      "|words|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-8b814c4c70af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'explode' is not defined"
     ]
    }
   ],
   "source": [
    "y.select(explode(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1=spark.readStream.schema(\"_col0 STRING, _col1 INTEGER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.streaming.DataStreamReader"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkStreaming = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df=sparkStreaming.readStream.csv(\"C:/Users/pg186028/Documents/DataVisualization/sparkStream\",\n",
    "                                        (\"col0 STRING, col1 INTEGER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df=spark.readStream.schema(\"col0 STRING, col1 INTEGER\").option(\"maxFilesPerTrigger\", 1).csv(\"C:/Users/pg186028/Documents/DataVisualization/sparkStream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql1=stream_df.groupBy(\"col0\").sum(\"col1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = sql1.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "static=spark.read.csv(\"C:/Users/pg186028/Documents/DataVisualization/sparkStream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,StringType,true),StructField(_c1,StringType,true)))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaFile=static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkStreaming = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df=spark.readStream.schema(schemaFile).option(\"maxFilesPerTrigger\", 1).csv(\"C:/Users/pg186028/Documents/DataVisualization/sparkStream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = sql1.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stream_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkStreaming2 = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_json=sparkStreaming2.read.json(\"C:/Users/pg186028/Documents/DataVisualization/part-00008-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema=spark_json.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "setJsonStream=sparkStreaming2.readStream.schema(json_schema).option(\"maxFilesPerTrigger\", 1).csv(\"C:/Users/pg186028/Documents/GitHub/Spark-The-Definitive-Guide/data/activity-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql1=setJsonStream.groupby('gt').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonQuery=sql1.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Getting offsets from FileStreamSource[file:/C:/Users/pg186028/Documents/GitHub/Spark-The-Definitive-Guide/data/activity-data]',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+-------------+-------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|            x|            y|            z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+-------------+-------------+\n",
      "|1424686734996|1424688581050172242|nexus4_2|    7|nexus4|   g|stand| -3.814697E-4|  0.019470215|  0.024795532|\n",
      "|1424686735195|1424688581250214967|nexus4_2|   47|nexus4|   g|stand| -0.008926392|  -0.04675293|  0.008773804|\n",
      "|1424686735398|1424688581451783570|nexus4_2|   87|nexus4|   g|stand| 0.0017547607| 0.0045166016|-0.0061798096|\n",
      "|1424686735600|1424686733604187998|nexus4_1|  120|nexus4|   g|stand| 0.0024719238| -0.035812378| -0.024154663|\n",
      "|1424686735803|1424686733807312998|nexus4_1|  160|nexus4|   g|stand| -7.324219E-4|   0.02079773| -0.005996704|\n",
      "|1424686736001|1424688582056031617|nexus4_2|  207|nexus4|   g|stand|  0.002822876| -0.016845703| -0.020065308|\n",
      "|1424686736206|1424686734208649668|nexus4_1|  240|nexus4|   g|stand| 0.0024719238| 0.0069122314| -6.561279E-4|\n",
      "|1424686736410|1424686734414917978|nexus4_1|  281|nexus4|   g|stand| 0.0024719238|  0.007980347|  -0.00920105|\n",
      "|1424686736614|1424686734601288828|nexus4_1|  318|nexus4|   g|stand|-0.0018005371|-0.0027008057| 0.0025482178|\n",
      "|1424686736809|1424688582861695679|nexus4_2|  367|nexus4|   g|stand|  0.002822876| 0.0034484863|-0.0051116943|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+-------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_json.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkStreaming2 = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'setMaster'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4657c25c59fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msparkStreaming2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[*]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'setMaster'"
     ]
    }
   ],
   "source": [
    "sparkStreaming2.sparkContext.setMaster(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pySparksqLite_test\")\\\n",
    ".config('spark.jars.packages', 'C:/spark/spark-2.3.0-bin-hadoop2.7/jars/sqlite-jdbc-3.20.0.jar').getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.repl.local.jars',\n",
       "  'file:///C:/Users/pg186028/Documents/DataVisualization/sqlite-jdbc-3.27.2.jar'),\n",
       " ('spark.jars',\n",
       "  'file:///C:/Users/pg186028/Documents/DataVisualization/sqlite-jdbc-3.27.2.jar'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://WINPG186028-3FP.TD.TERADATA.COM:4040\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"jdbc\").\\\n",
    "options(url =\"jdbc:sqlite:C:/sqlite-tools-win32-x86-3290000/my-sqlite.db\",\n",
    "        driver=\"org.sqlite.JDBC\",\n",
    "        dbtable=\"(select DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count from flight_info)\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf=SparkConf()\n",
    "\n",
    "cnf.set(\"spark.jars\",\"C:\\\\Users\\pg186028\\Documents\\DataVisualization\\sqlite-jdbc-3.27.2.jar\")\n",
    "sparkX = SparkSession.builder.appName(\"pySparksqLite_test\").master(\"local[2]\").config(conf=cnf).getOrCreate()\n",
    "values = sparkX.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame([\n",
    "[1,1,4,500,1000]\n",
    ",[2,1,3,100,200]\n",
    ",[5,3,2,200,400]\n",
    ",[1,3,4,200,200]\n",
    "], columns=['id1','id2','id3','visit','investment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>id3</th>\n",
       "      <th>visit</th>\n",
       "      <th>investment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id1  id2  id3  visit  investment\n",
       "0    1    1    4    500        1000\n",
       "1    2    1    3    100         200\n",
       "2    5    3    2    200         400\n",
       "3    1    3    4    200         200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkX = SparkSession.builder.appName(\"explode\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bae74053c882>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msparkX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'DataFrame'"
     ]
    }
   ],
   "source": [
    "DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=sparkX.createDataFrame(x.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----+----------+\n",
      "|id1|id2|id3|visit|investment|\n",
      "+---+---+---+-----+----------+\n",
      "|  1|  1|  4|  500|      1000|\n",
      "|  2|  1|  3|  100|       200|\n",
      "|  5|  3|  2|  200|       400|\n",
      "|  1|  3|  4|  200|       200|\n",
      "+---+---+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sparkX.sparkContext.parallelize([1, 2, 3, 4], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(splitIndex, iterator): yield splitIndex, lambda iterator : len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=rdd.mapPartitionsWithIndex(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[{'0': 'HUMAN RESOURCES'}, {'1': 'CITY COUNCIL'}, {'1': 'TRANSPORTN'}, {'1': 'COMMUNITY DEVELOPMENT'}, {'1': 'INSPECTOR GEN'}, {'1': 'TREASURER'}, {'1': 'ADMIN HEARNG'}, {'2': 'STREETS & SAN'}, {'2': 'LAW'}, {'2': 'FINANCE'}, {'2': 'HUMAN RELATIONS'}, {'2': 'LICENSE APPL COMM'}, {'2': 'None'}, {'3': 'AVIATION'}, {'3': 'BUSINESS AFFAIRS'}, {'3': 'DISABILITIES'}, {'3': 'DoIT'}, {'4': 'WATER MGMNT'}, {'4': 'POLICE'}, {'4': 'FIRE'}, {'4': 'IPRA'}, {'4': 'POLICE BOARD'}, {'5': 'GENERAL SERVICES'}, {'5': 'FAMILY & SUPPORT'}, {'5': 'HEALTH'}, {'5': 'BOARD OF ELECTION'}, {'5': 'BOARD OF ETHICS'}, {'6': 'OEMC'}, {'6': \"MAYOR'S OFFICE\"}, {'6': 'BUILDINGS'}, {'6': 'PROCUREMENT'}, {'7': 'PUBLIC LIBRARY'}, {'7': 'CULTURAL AFFAIRS'}, {'7': 'ANIMAL CONTRL'}, {'7': 'CITY CLERK'}, {'7': 'BUDGET & MGMT'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': 'HUMAN RESOURCES'},\n",
       " {'1': 'CITY COUNCIL'},\n",
       " {'1': 'TRANSPORTN'},\n",
       " {'1': 'COMMUNITY DEVELOPMENT'},\n",
       " {'1': 'INSPECTOR GEN'},\n",
       " {'1': 'TREASURER'},\n",
       " {'1': 'ADMIN HEARNG'},\n",
       " {'2': 'STREETS & SAN'},\n",
       " {'2': 'LAW'},\n",
       " {'2': 'FINANCE'},\n",
       " {'2': 'HUMAN RELATIONS'},\n",
       " {'2': 'LICENSE APPL COMM'},\n",
       " {'2': 'None'},\n",
       " {'3': 'AVIATION'},\n",
       " {'3': 'BUSINESS AFFAIRS'},\n",
       " {'3': 'DISABILITIES'},\n",
       " {'3': 'DoIT'},\n",
       " {'4': 'WATER MGMNT'},\n",
       " {'4': 'POLICE'},\n",
       " {'4': 'FIRE'},\n",
       " {'4': 'IPRA'},\n",
       " {'4': 'POLICE BOARD'},\n",
       " {'5': 'GENERAL SERVICES'},\n",
       " {'5': 'FAMILY & SUPPORT'},\n",
       " {'5': 'HEALTH'},\n",
       " {'5': 'BOARD OF ELECTION'},\n",
       " {'5': 'BOARD OF ETHICS'},\n",
       " {'6': 'OEMC'},\n",
       " {'6': \"MAYOR'S OFFICE\"},\n",
       " {'6': 'BUILDINGS'},\n",
       " {'6': 'PROCUREMENT'},\n",
       " {'7': 'PUBLIC LIBRARY'},\n",
       " {'7': 'CULTURAL AFFAIRS'},\n",
       " {'7': 'ANIMAL CONTRL'},\n",
       " {'7': 'CITY CLERK'},\n",
       " {'7': 'BUDGET & MGMT'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HUMAN RESOURCES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CITY COUNCIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRANSPORTN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>COMMUNITY DEVELOPMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>INSPECTOR GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>TREASURER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>ADMIN HEARNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>STREETS &amp; SAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>LAW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>FINANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>HUMAN RELATIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>LICENSE APPL COMM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>BUSINESS AFFAIRS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>DISABILITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>DoIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>WATER MGMNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>POLICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>FIRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>IPRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>POLICE BOARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>GENERAL SERVICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>FAMILY &amp; SUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>BOARD OF ELECTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>BOARD OF ETHICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>OEMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>MAYOR'S OFFICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "      <td>BUILDINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6</td>\n",
       "      <td>PROCUREMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7</td>\n",
       "      <td>PUBLIC LIBRARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7</td>\n",
       "      <td>CULTURAL AFFAIRS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7</td>\n",
       "      <td>ANIMAL CONTRL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7</td>\n",
       "      <td>CITY CLERK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7</td>\n",
       "      <td>BUDGET &amp; MGMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                      1\n",
       "0   0        HUMAN RESOURCES\n",
       "1   1           CITY COUNCIL\n",
       "2   1             TRANSPORTN\n",
       "3   1  COMMUNITY DEVELOPMENT\n",
       "4   1          INSPECTOR GEN\n",
       "5   1              TREASURER\n",
       "6   1           ADMIN HEARNG\n",
       "7   2          STREETS & SAN\n",
       "8   2                    LAW\n",
       "9   2                FINANCE\n",
       "10  2        HUMAN RELATIONS\n",
       "11  2      LICENSE APPL COMM\n",
       "12  2                   None\n",
       "13  3               AVIATION\n",
       "14  3       BUSINESS AFFAIRS\n",
       "15  3           DISABILITIES\n",
       "16  3                   DoIT\n",
       "17  4            WATER MGMNT\n",
       "18  4                 POLICE\n",
       "19  4                   FIRE\n",
       "20  4                   IPRA\n",
       "21  4           POLICE BOARD\n",
       "22  5       GENERAL SERVICES\n",
       "23  5       FAMILY & SUPPORT\n",
       "24  5                 HEALTH\n",
       "25  5      BOARD OF ELECTION\n",
       "26  5        BOARD OF ETHICS\n",
       "27  6                   OEMC\n",
       "28  6         MAYOR'S OFFICE\n",
       "29  6              BUILDINGS\n",
       "30  6            PROCUREMENT\n",
       "31  7         PUBLIC LIBRARY\n",
       "32  7       CULTURAL AFFAIRS\n",
       "33  7          ANIMAL CONTRL\n",
       "34  7             CITY CLERK\n",
       "35  7          BUDGET & MGMT"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([ (k,v) for x in X for (k, v) in x.items() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[[1,2.0444949493494923923],[1,3.2131203392492348234],[2,4000009.5934923492],[2,5000009.9093099787676767]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=sparkX.createDataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=y.withColumnRenamed(\"X\",\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| _1|                _2|\n",
      "+---+------------------+\n",
      "|  1|2.0444949493494926|\n",
      "|  1| 3.213120339249235|\n",
      "|  2| 4000009.593492349|\n",
      "|  2| 5000009.909309979|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _SUM\n",
    "from pyspark.sql.functions import round as _ROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1=z.groupBy(\"_1\").agg(_ROUND(_SUM(\"_2\"),3)).alias(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "| _1|round(sum(_2), 3)|\n",
      "+---+-----------------+\n",
      "|  1|            5.258|\n",
      "|  2|      9000019.503|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-25 21:30:34\n",
      "Default number of partitions:1\n",
      "Number of partitions  after repartitioning:8\n",
      "Partition column :None\n",
      "[70, 1795, 3161, 1812, 19338, 2239, 2413, 1235]\n",
      "partition Structure\n",
      "[{'0': 'HUMAN RESOURCES'}, {'1': 'CITY COUNCIL'}, {'1': 'TRANSPORTN'}, {'1': 'COMMUNITY DEVELOPMENT'}, {'1': 'INSPECTOR GEN'}, {'1': 'TREASURER'}, {'1': 'ADMIN HEARNG'}, {'2': 'STREETS & SAN'}, {'2': 'LAW'}, {'2': 'FINANCE'}, {'2': 'HUMAN RELATIONS'}, {'2': 'LICENSE APPL COMM'}, {'2': 'None'}, {'3': 'AVIATION'}, {'3': 'BUSINESS AFFAIRS'}, {'3': 'DISABILITIES'}, {'3': 'DoIT'}, {'4': 'WATER MGMNT'}, {'4': 'POLICE'}, {'4': 'FIRE'}, {'4': 'IPRA'}, {'4': 'POLICE BOARD'}, {'5': 'GENERAL SERVICES'}, {'5': 'FAMILY & SUPPORT'}, {'5': 'HEALTH'}, {'5': 'BOARD OF ELECTION'}, {'5': 'BOARD OF ETHICS'}, {'6': 'OEMC'}, {'6': \"MAYOR'S OFFICE\"}, {'6': 'BUILDINGS'}, {'6': 'PROCUREMENT'}, {'7': 'PUBLIC LIBRARY'}, {'7': 'CULTURAL AFFAIRS'}, {'7': 'ANIMAL CONTRL'}, {'7': 'CITY CLERK'}, {'7': 'BUDGET & MGMT'}]\n",
      "    a                      b\n",
      "0   0        HUMAN RESOURCES\n",
      "1   1           CITY COUNCIL\n",
      "2   1             TRANSPORTN\n",
      "3   1  COMMUNITY DEVELOPMENT\n",
      "4   1          INSPECTOR GEN\n",
      "5   1              TREASURER\n",
      "6   1           ADMIN HEARNG\n",
      "7   2          STREETS & SAN\n",
      "8   2                    LAW\n",
      "9   2                FINANCE\n",
      "10  2        HUMAN RELATIONS\n",
      "11  2      LICENSE APPL COMM\n",
      "12  2                   None\n",
      "13  3               AVIATION\n",
      "14  3       BUSINESS AFFAIRS\n",
      "15  3           DISABILITIES\n",
      "16  3                   DoIT\n",
      "17  4            WATER MGMNT\n",
      "18  4                 POLICE\n",
      "19  4                   FIRE\n",
      "20  4                   IPRA\n",
      "21  4           POLICE BOARD\n",
      "22  5       GENERAL SERVICES\n",
      "23  5       FAMILY & SUPPORT\n",
      "24  5                 HEALTH\n",
      "25  5      BOARD OF ELECTION\n",
      "26  5        BOARD OF ETHICS\n",
      "27  6                   OEMC\n",
      "28  6         MAYOR'S OFFICE\n",
      "29  6              BUILDINGS\n",
      "30  6            PROCUREMENT\n",
      "31  7         PUBLIC LIBRARY\n",
      "32  7       CULTURAL AFFAIRS\n",
      "33  7          ANIMAL CONTRL\n",
      "34  7             CITY CLERK\n",
      "35  7          BUDGET & MGMT\n",
      "data\n",
      "+--------------------+--------------+\n",
      "|          Department|             X|\n",
      "+--------------------+--------------+\n",
      "|     HUMAN RESOURCES|     5313360.0|\n",
      "|        CITY COUNCIL| 2.284304856E7|\n",
      "|          TRANSPORTN|  9.13049476E7|\n",
      "|COMMUNITY DEVELOP...|  1.79103032E7|\n",
      "|       INSPECTOR GEN|     4680780.0|\n",
      "|           TREASURER|     1991577.0|\n",
      "|        ADMIN HEARNG|     2996364.0|\n",
      "|       STREETS & SAN| 1.483405852E8|\n",
      "|                 LAW|  3.09241238E7|\n",
      "|             FINANCE|  4.08122384E7|\n",
      "|     HUMAN RELATIONS|     1457052.0|\n",
      "|   LICENSE APPL COMM|       71292.0|\n",
      "|            AVIATION| 1.119751528E8|\n",
      "|    BUSINESS AFFAIRS|  1.24684176E7|\n",
      "|        DISABILITIES|     2259888.0|\n",
      "|                DoIT|     9866184.0|\n",
      "|         WATER MGMNT|1.5744227816E8|\n",
      "|              POLICE|1.0928495046E9|\n",
      "|                FIRE|4.5898020856E8|\n",
      "|                IPRA|     7385256.0|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "2019-09-25 21:30:58\n"
     ]
    }
   ],
   "source": [
    "# ALL about spark partitions\n",
    "# partition concept in spark\n",
    "# repartition data using a column\n",
    "# not created any functions this is for learning\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.sql.functions import regexp_replace,col\n",
    "from pyspark.sql.functions import sum as _SUM\n",
    "from pyspark.sql.functions import round  as Round\n",
    "\n",
    "\n",
    "# start time of script , this is to see exact time of execution\n",
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "sparkp = SparkSession.builder.appName(\"partitions\").master(\"local[*]\").getOrCreate()\n",
    "chicago=sparkp.read.csv(\"C:/Users/pg186028/Documents/DataVisualization/chicago.csv\"\n",
    "                , inferSchema=True, header=True, nullValue=\"XX\")\n",
    "chicago2 = chicago.repartition(8, f.col(\"Department\"))\n",
    "\n",
    "# initial check\n",
    "print(\"Default number of partitions:{}\".format(chicago.rdd.getNumPartitions()))\n",
    "print(\"Number of partitions  after repartitioning:{}\".format(chicago2.rdd.getNumPartitions()))\n",
    "print(\"Partition column :{}\".format(chicago2.rdd.partitioner))\n",
    "\n",
    "# check length (count) of each partition\n",
    "print(chicago2.rdd.glom().map(len).collect())\n",
    "\n",
    "# now lets see how data is distributed in partitions\n",
    "print(\"partition Structure\")\n",
    "\n",
    "def f(index,it):\n",
    "    for x in it:\n",
    "        yield {str(index) : str(x[\"Department\"])}\n",
    "rdd_partition = chicago2.rdd.mapPartitionsWithIndex(f).collect()\n",
    "\n",
    "# take distinct values key value of each partition and put it in a list.\n",
    "x=[]\n",
    "for i in rdd_partition:\n",
    "    if i not in x:\n",
    "        x.append(i)\n",
    "print(x)\n",
    "\n",
    "# let's see which partition has what data\n",
    "rdd_partiton_key=pd.DataFrame([(k, v) for p in x for (k, v) in p.items()],columns=[\"a\", \"b\"])\n",
    "print(rdd_partiton_key)\n",
    "\n",
    "print(\"data\")\n",
    "\n",
    "chicago3=chicago2.withColumn(\"Employee Annual Salary\" ,regexp_replace(col(\"Employee Annual Salary\"), '\\\\$','')).\\\n",
    "where(col(\"Department\").isNotNull())\n",
    "#chicago3.head(5)\n",
    "\n",
    "chicago4=chicago3.groupby(\"Department\").agg(Round(_SUM('Employee Annual Salary'),2).alias(\"X\"))\n",
    "chicago4.show()\n",
    "\n",
    "print(type(chicago4))\n",
    "print(type(chicago3))\n",
    "\n",
    "\n",
    "# start time of script , this is to see end time of execution\n",
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago4=chicago3.groupby(\"Department\").agg(Round(_SUM('Employee Annual Salary'),2).alias(\"X\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|          Department|             X|\n",
      "+--------------------+--------------+\n",
      "|     HUMAN RESOURCES|     5313360.0|\n",
      "|        CITY COUNCIL| 2.284304856E7|\n",
      "|          TRANSPORTN|  9.13049476E7|\n",
      "|COMMUNITY DEVELOP...|  1.79103032E7|\n",
      "|       INSPECTOR GEN|     4680780.0|\n",
      "|           TREASURER|     1991577.0|\n",
      "|        ADMIN HEARNG|     2996364.0|\n",
      "|       STREETS & SAN| 1.483405852E8|\n",
      "|                 LAW|  3.09241238E7|\n",
      "|             FINANCE|  4.08122384E7|\n",
      "|     HUMAN RELATIONS|     1457052.0|\n",
      "|   LICENSE APPL COMM|       71292.0|\n",
      "|            AVIATION| 1.119751528E8|\n",
      "|    BUSINESS AFFAIRS|  1.24684176E7|\n",
      "|        DISABILITIES|     2259888.0|\n",
      "|                DoIT|     9866184.0|\n",
      "|         WATER MGMNT|1.5744227816E8|\n",
      "|              POLICE|1.0928495046E9|\n",
      "|                FIRE|4.5898020856E8|\n",
      "|                IPRA|     7385256.0|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chicago4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|          Department|   round(X, 2)|\n",
      "+--------------------+--------------+\n",
      "|     HUMAN RESOURCES|     5313360.0|\n",
      "|        CITY COUNCIL| 2.284304856E7|\n",
      "|          TRANSPORTN|  9.13049476E7|\n",
      "|COMMUNITY DEVELOP...|  1.79103032E7|\n",
      "|       INSPECTOR GEN|     4680780.0|\n",
      "|           TREASURER|     1991577.0|\n",
      "|        ADMIN HEARNG|     2996364.0|\n",
      "|       STREETS & SAN| 1.483405852E8|\n",
      "|                 LAW|  3.09241238E7|\n",
      "|             FINANCE|  4.08122384E7|\n",
      "|     HUMAN RELATIONS|     1457052.0|\n",
      "|   LICENSE APPL COMM|       71292.0|\n",
      "|            AVIATION| 1.119751528E8|\n",
      "|    BUSINESS AFFAIRS|  1.24684176E7|\n",
      "|        DISABILITIES|     2259888.0|\n",
      "|                DoIT|     9866184.0|\n",
      "|         WATER MGMNT|1.5744227816E8|\n",
      "|              POLICE|1.0928495046E9|\n",
      "|                FIRE|4.5898020856E8|\n",
      "|                IPRA|     7385256.0|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chicago4.select(\"Department\",Round(col(\"X\"),2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+----------------------+\n",
      "|                Name|      Position Title|     Department|Employee Annual Salary|\n",
      "+--------------------+--------------------+---------------+----------------------+\n",
      "| ALEXANDER,  WENDY R|           RECRUITER|HUMAN RESOURCES|              75048.00|\n",
      "| ANGELETTI,  DIANE E|           CLERK III|HUMAN RESOURCES|              39096.00|\n",
      "|  ANSON,  BENJAMIN W|CLASSIFICATION & ...|HUMAN RESOURCES|              66768.00|\n",
      "|BATORSKI,  CHRISTINA| DEPUTY COMMISSIONER|HUMAN RESOURCES|             116604.00|\n",
      "| BOWMAN,  BENJAMIN A|ASSOC CLASSIFICAT...|HUMAN RESOURCES|              58284.00|\n",
      "|BRANTLEY KIRBY,  ...|CLASSIFICATION & ...|HUMAN RESOURCES|              90288.00|\n",
      "|       BURNS,  LEO J|MANAGING DEPUTY C...|HUMAN RESOURCES|             127824.00|\n",
      "|   CASSELL,  JACQLYN|           RECRUITER|HUMAN RESOURCES|              81228.00|\n",
      "|          CHOI,  SOO|COMMISSIONER OF H...|HUMAN RESOURCES|             151572.00|\n",
      "|   COLWELL,  TRACY P|TRAINING & DEVELO...|HUMAN RESOURCES|              82668.00|\n",
      "|    CONDON,  KRISTIN|HR RECORDS SPECIA...|HUMAN RESOURCES|              48852.00|\n",
      "|   COULTER,  CRAIG A|           RECRUITER|HUMAN RESOURCES|              92784.00|\n",
      "|    DECKER,  GRACE A|HR RECORDS SPECIA...|HUMAN RESOURCES|              54108.00|\n",
      "|     DELLO,  DIANE A|    INQUIRY AIDE III|HUMAN RESOURCES|              40932.00|\n",
      "|DEMOPOULOS,  PANA...|           RECRUITER|HUMAN RESOURCES|              96840.00|\n",
      "|DEUCHER,  ELIZABE...|  EEO INVESTIGATOR I|HUMAN RESOURCES|              55464.00|\n",
      "|       DIAZ,  GLADYS|           RECRUITER|HUMAN RESOURCES|              81228.00|\n",
      "|EASTER,  PHYLLINIS M|           RECRUITER|HUMAN RESOURCES|              88788.00|\n",
      "|        ENG,  JOSEPH|SENIOR PROGRAMMER...|HUMAN RESOURCES|              90288.00|\n",
      "|     FISHER,  IRIS M|CLASSIFICATION & ...|HUMAN RESOURCES|              90288.00|\n",
      "+--------------------+--------------------+---------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chicago3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago5=chicago4.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35 entries, 0 to 34\n",
      "Data columns (total 2 columns):\n",
      "Department    35 non-null object\n",
      "X             35 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 640.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "chicago5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago5[\"X\"]=chicago5[\"X\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35 entries, 0 to 34\n",
      "Data columns (total 2 columns):\n",
      "Department    35 non-null object\n",
      "X             35 non-null int32\n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 500.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "chicago5.info() # look at the change in memory usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', 'WINPG186028-3FP.TD.TERADATA.COM'),\n",
       " ('spark.app.id', 'local-1569512807811'),\n",
       " ('spark.driver.port', '56998'),\n",
       " ('spark.app.name', 'Word Count'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkp.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"A quick , very quick little brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkp = SparkSession.builder.appName(\"Word Count\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sparkp.sparkContext.parallelize([string]).flatMap(lambda x : x.split(\" \")).map(lambda w:(w,1)).reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(8) PythonRDD[17] at collect at <ipython-input-12-4ff2a94bd2aa>:1 []\\n |  MapPartitionsRDD[16] at mapPartitions at PythonRDD.scala:122 []\\n |  ShuffledRDD[15] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(8) PairwiseRDD[14] at reduceByKey at <ipython-input-11-ae271ce96034>:1 []\\n    |  PythonRDD[13] at reduceByKey at <ipython-input-11-ae271ce96034>:1 []\\n    |  ParallelCollectionRDD[12] at parallelize at PythonRDD.scala:175 []'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'dependencies'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5166dc863e69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'dependencies'"
     ]
    }
   ],
   "source": [
    "rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WINPG186028-3FP.TD.TERADATA.COM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ed02fdb710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from pyspark import SparkConf\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2705f45e4e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf.set(\"spark.jars\", \"C:\\\\Users\\pg186028\\Documents\\DataVisualization\\mongo-spark-connector_2.11-2.4.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkMongo = SparkSession.builder.appName(\"pySparkMogoDb\").master(\"local[2]\")\\\n",
    "    .config(\"spark.mongodb.input.uri=mongodb://127.0.0.1:27017/school.scores\") \\\n",
    "    .config(\"spark.mongodb.output.uri=mongodb://127.0.0.1:27017/school.scores\") \\\n",
    "    .config('spark.driver.extraClassPath', working_directory) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "daf=spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").options(url=\"mongodb://127.0.0.1:27017/school.scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameReader' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-74d8ebbd54c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdaf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrameReader' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "daf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=daf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o118.load.\n: java.lang.ClassNotFoundException: Failed to find data source: mongo. Please find packages at http://spark.apache.org/third-party-projects.html\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:635)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:618)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:618)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:618)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:618)\r\n\tat scala.util.Try.orElse(Try.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:618)\r\n\t... 13 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-08f9d0828de0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o118.load.\n: java.lang.ClassNotFoundException: Failed to find data source: mongo. Please find packages at http://spark.apache.org/third-party-projects.html\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:635)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:618)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:618)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:618)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:618)\r\n\tat scala.util.Try.orElse(Try.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:618)\r\n\t... 13 more\r\n"
     ]
    }
   ],
   "source": [
    "x.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
